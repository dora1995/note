任何需求都可以做成你的亮点，只需要你把它的量级扩大，想得更严谨

实现拖拽，需要监听三个事件：dragover，dragleave，drop

可以做一个进度条，axios有专门的onUploadProgress，通过计算progress.loaded / progress.total

判断上传文件的格式，文件的文件流十六进制数据都有特定的代码段，百度文件头信息

```
async blobToString (blob) {
  return new Promise(resolve => {
    const reader = new FileReader()
    reader.onload = function () {
      const ret = reader.result.split('').map(v => v.toString(16).toUpperCase()).join('')
      resolve(ret)
    }
    reader.readAsBinaryString(blob)
    }
  )
}
```

用md5做文件的唯一标识，不能只用名字；用web-worker计算

- **js-spark-md5**，支持增量计算，npm下载后然后拷出来
- 文件计算很慢，还有为了不阻塞进程，使用worker多线程计算，通过postMessage和onMessage进行信息交互
- filerReader的使用，readAsArrayBuffer

断点续传，比如有一个2g的文件，在中间网络断开了，重新上传又要重新开始，这是不好的

- 切片，`1m = 1024*1024`
- 启动worker，调用md5

```
// worker代码

self.importScript('spark-md5.min.js')

self.onmessage = e => {
  const { chunk } = e.data
  const spark = new self.SparkMD5.ArrayBuffer()
  let progress = 0
  let count = 0
  const loadNext = index => {
    const reader = new FileReader()
    reader.readAsArrayBuffer(chunks[index].file)
    reader.onload = e => {
      count ++
      spark.md5(e.target.result)
      if (count == chunks.length) {
        self.postMessage({
          progress: 100,
          hash: spark.end()
        })
      } else {
        progress += 100 / chunks.length
        self.postMessage({
          progress
        })
        loadNext(count)
      }
    }
  }
}
```

利用浏览器空闲时间处理

```
async calculateHashIdle() {
  const chunks = this.chunks
  return new Promise(resolve=>{
    const spark = new sparkMD5.ArrayBuffer()
    let count = 0 
    const appendToSpark = async file=>{
      return new Promise(resolve=>{
        const reader = new FileReader()
        reader.readAsArrayBuffer(file)
        reader.onload = e=>{
          spark.append(e.target.result)
          resolve()
        }
      })
    }
    const workLoop = async deadline=>{
      // timeRemaining获取当前帧的剩余时间
      while(count<chunks.length && deadline.timeRemaining()>1){
        // 空闲时间，且有任务
        await appendToSpark(chunks[count].file)
        count++
        if(count<chunks.length){
          this.hashProgress = Number(
            ((100*count)/chunks.length).toFixed(2)
          )
        }else{
          this.hashProgress = 100
          resolve(spark.end())
        }
      }
      window.requestIdleCallback(workLoop)
    }
    // 浏览器一旦空闲，就会调用workLoop
    window.requestIdleCallback(workLoop)
  })
}
```

抽样hash计算：不算全量，布隆过滤器，损失一小部分的精度，换取效率

第一个2m，最后一个区域数据全要，中间的，取前中后各2字节




